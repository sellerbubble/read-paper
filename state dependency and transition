Deepseek R1发现，LLM在通过RL训练之后可以实现aha moment，即出现了自主反思，重新评估等推理能力。具体表现为通过“wait”，“but”等词语推翻之前的推理内容而开启新的推理过程。
这种现象其实揭示了LLM的【弱路径依赖性】，即拥有推理能力的LLM在生成文本的时候并不会受到之前生成的内容的强烈影响。相比较之下，下围棋或者玩游戏或者机器人的模型则会受到之前路径的强烈约束。
这也就是为什么我认为LLM的RL中Process Reward Model方法不如Outcome Reward Model，因为在推理过程中，LLM某个推理步骤所导致的【微观状态转移】对最终的【宏观推理路径】的影响很可能是趋近于零的，process reward就会因此失效。
而视频生成则和下围棋很像，某一帧图像（或者某一次生成得到的状态）是需要严格受到上一帧（上一状态）中的时空关系的约束，因此视频生成具有近似马尔科夫的【强路径依赖性】。PRM或者类似的dense reward思路会更加有效。
同时，我们能否受到LLM的弱路径依赖的启发，来设计更好的视频生成思路呢？
关键帧生成可能是一个比较promising的做法。本质是通过插值来使得状态变化更加平滑。同时，如果可以反复修改关键帧，也许可以实现类似COT的效果。
