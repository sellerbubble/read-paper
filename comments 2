【MetaMorph: Multimodal Understanding and Generation via Instruction Tuning】
验证了image generation和understanding可以相互促进

【Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models】
通过SAE的方法验证了，在ViT中也存在feature。siglip中的feature可以包含abstract concept。

【Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation】
Understanding can benefit from generation, but this benefit depends on the alignment between the vision input and output spaces： 通过轻微改变input vision embedding，来使得misalignment并观察到scores下降
knowledge transfer between gene and under: 通过构造数据集，来创造出独特的数据集特性：只在生成数据集中包含某些信息，并验证理解任务上模型的表现

【Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification】
不同模态的数据不一定要share latent space，重要的也许不是latent space，而是to latent space的过程。能否让理解和生成是互相可逆的两个过程。是否在UMM中已经存在了某种分布上的可逆，或者某种share latent space

【FROZEN TRANSFORMERS IN LANGUAGE MODELS
ARE EFFECTIVE VISUAL ENCODER LAYERS】
在vit后面拼接一层frozen llm layer，能够使得vit的下游任务表现更好。
经过研究发现，图片主要区域的feature activation增大，但是cls token的attn scores并没有显著的特点。这说明llm layer放大了原本vit tokens中的有用信息，这间接benefit了cls token。
这个现象可能可以体现出，llm 相比于vit的额外能力，可能是对于隐藏特征的挖掘。这来自于llm 的MLP而非attn，因为llm能够适应量级庞大的text token，这赋予了llm很强的对token的处理能力。
只要让vit token符合某种llm能够理解的representation，就能够通过llm来放大vit token的某些特征。
这对于VLM和UMM也有启发。说明其他模态的token很有可能是落入了llm 能够理解的连续的文本空间，从而因为某种程度类似text而获得了收益。
而【Frozen Language Models are Gradient Coherence Rectifiers in Vision
Transformers】提供了一个更理论的视角，llm通过影响梯度来帮助优化vit。llm layer增加了gradient coherence，即不同训练样本对应的梯度的方向一致性增加。这也许同样说明了，llm 会放大token中的某些有用feature。
从而使得优化的方向更好，减少了优化的噪音。

1：image a -> text a
2：text a -> image b 
3：image b -> text c
理解的过程中，representation本身及其转变很重要。生成的过程中，representation不再重要，可能是由于生成的VAE latents本身就是poor representation。
生成和理解的representation的大差异，导致了对同一data point，理解和生成不一致。1和2并不是可逆的，就会出现3的情况
