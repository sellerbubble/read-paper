【MetaMorph: Multimodal Understanding and Generation via Instruction Tuning】
验证了image generation和understanding可以相互促进

【Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models】
通过SAE的方法验证了，在ViT中也存在feature。siglip中的feature可以包含abstract concept。

【Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation】
Understanding can benefit from generation, but this benefit depends on the alignment between the vision input and output spaces： 通过轻微改变input vision embedding，来使得misalignment并观察到scores下降
knowledge transfer between gene and under: 通过构造数据集，来创造出独特的数据集特性：只在生成数据集中包含某些信息，并验证理解任务上模型的表现

【Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification】
不同模态的数据不一定要share latent space，重要的也许不是latent space，而是to latent space的过程。能否让理解和生成是互相可逆的两个过程。是否在UMM中已经存在了某种分布上的可逆，或者某种share latent space

【FROZEN TRANSFORMERS IN LANGUAGE MODELS
ARE EFFECTIVE VISUAL ENCODER LAYERS】
在vit后面拼接一层frozen llm layer，能够使得vit的下游任务表现更好。
经过研究发现，图片主要区域的feature activation增大，但是cls token的attn scores并没有显著的特点。这说明llm layer放大了原本vit tokens中的有用信息，这间接benefit了cls token。
这个现象可能可以体现出，llm 相比于vit的额外能力，可能是对于隐藏特征的挖掘。这来自于llm 的MLP而非attn，因为llm能够适应量级庞大的text token，这赋予了llm很强的对token的处理能力。
只要让vit token符合某种llm能够理解的representation，就能够通过llm来放大vit token的某些特征。
这对于VLM和UMM也有启发。说明其他模态的token很有可能是落入了llm 能够理解的连续的文本空间，从而因为某种程度类似text而获得了收益。
而【Frozen Language Models are Gradient Coherence Rectifiers in Vision
Transformers】提供了一个更理论的视角，llm通过影响梯度来帮助优化vit。llm layer增加了gradient coherence，即不同训练样本对应的梯度的方向一致性增加。这也许同样说明了，llm 会放大token中的某些有用feature。
从而使得优化的方向更好，减少了优化的噪音。

【UNIRL】
1：image a -> text a
2：text a -> image b 
3：image b -> text c
理解的过程中，representation本身及其转变很重要。生成的过程中，representation不再重要，可能是由于生成的VAE latents本身就是poor representation。
生成和理解的representation的大差异，导致了对同一data point，理解和生成不一致。1和2并不是可逆的，就会出现3的情况。是否需要让模型针对p(image a | text a) 和 p（text a | image a)进行统一建模
show-o 对理解和生成使用magvit统一编码，即使用codebook离散编码，生成的时候采用discrete diffusion。这种方法使得representation一致。
unirl的核心就在于，生成图像token并用于回答理解问题时，针对理解问题的梯度可以回传到生成图像的token。但是从直觉上让理解问题直接通过梯度优化生成，好像很难想通。但是对于生成和理解统一representation的模型来说，似乎让生成和理解作为同一个任务来训练是有好处的。因为生成的gt不再是另一张图片，而是来自text gt的implicit supervision。
t2I和MMu的能力不匹配，是否：如果t2i强于mmu，就先理解再生成，这样生成的loss可以回传到理解部分，促使模型的理解能力提升。宗旨是，让弱的能力受到强的能力的监督，因为强的能力确保了其本身的可靠性，所以误差更多来自于弱的能力。但是可能隐患是破坏了强的能力，因此需要思考如何将梯度隔离，只优化和弱的能力相关的token。也许可以detach ？

如果1到2的过程不对，或者2到3的过程不对，那么说明理解和生成能力之间有差距。
理解的起点是image encoder的representation，在理解的过程中逐渐分布转移到了能够被理解的space。而生成则是从噪音中获得了representation。这是两种from distribution to distribution的转移过程。
两种转移过程相互之间是隔离的，割裂的。
