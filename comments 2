【MetaMorph: Multimodal Understanding and Generation via Instruction Tuning】
验证了image generation和understanding可以相互促进

【Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models】
通过SAE的方法验证了，在ViT中也存在feature。siglip中的feature可以包含abstract concept。

【Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation】
Understanding can benefit from generation, but this benefit depends on the alignment between the vision input and output spaces： 通过轻微改变input vision embedding，来使得misalignment并观察到scores下降
knowledge transfer between gene and under: 通过构造数据集，来创造出独特的数据集特性：只在生成数据集中包含某些信息，并验证理解任务上模型的表现

【Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification】
不同模态的数据不一定要share latent space，重要的也许不是latent space，而是to latent space的过程。能否让理解和生成是互相可逆的两个过程。是否在UMM中已经存在了某种分布上的可逆，或者某种share latent space

【FROZEN TRANSFORMERS IN LANGUAGE MODELS
ARE EFFECTIVE VISUAL ENCODER LAYERS】
在vit后面拼接一层frozen llm layer，能够使得vit的下游任务表现更好。
经过研究发现，图片主要区域的feature activation增大，但是cls token的attn scores并没有显著的特点。这说明llm layer放大了原本vit tokens中的有用信息，这间接benefit了cls token。
这个现象可能可以体现出，llm 相比于vit的额外能力，可能是对于隐藏特征的挖掘。这来自于llm 的MLP而非attn，因为llm能够适应量级庞大的text token，这赋予了llm很强的对token的处理能力。
只要让vit token符合某种llm能够理解的representation，就能够通过llm来放大vit token的某些特征。
这对于VLM和UMM也有启发。说明其他模态的token很有可能是落入了llm 能够理解的连续的文本空间，从而因为某种程度类似text而获得了收益。
而【Frozen Language Models are Gradient Coherence Rectifiers in Vision
Transformers】提供了一个更理论的视角，llm通过影响梯度来帮助优化vit。llm layer增加了gradient coherence，即不同训练样本对应的梯度的方向一致性增加。这也许同样说明了，llm 会放大token中的某些有用feature。
从而使得优化的方向更好，减少了优化的噪音。

【UNIRL】
1：image a -> text a
2：text a -> image b 
3：image b -> text c
理解的过程中，representation本身及其转变很重要。生成的过程中，representation不再重要，可能是由于生成的VAE latents本身就是poor representation。
生成和理解的representation的大差异，导致了对同一data point，理解和生成不一致。1和2并不是可逆的，就会出现3的情况。是否需要让模型针对p(image a | text a) 和 p（text a | image a)进行统一建模
show-o 对理解和生成使用magvit统一编码，即使用codebook离散编码，生成的时候采用discrete diffusion。这种方法使得representation一致。
unirl的核心就在于，生成图像token并用于回答理解问题时，针对理解问题的梯度可以回传到生成图像的token。但是从直觉上让理解问题直接通过梯度优化生成，好像很难想通。但是对于生成和理解统一representation的模型来说，似乎让生成和理解作为同一个任务来训练是有好处的。因为生成的gt不再是另一张图片，而是来自text gt的implicit supervision。
t2I和MMu的能力不匹配，是否：如果t2i强于mmu，就先理解再生成，这样生成的loss可以回传到理解部分，促使模型的理解能力提升。宗旨是，让弱的能力受到强的能力的监督，因为强的能力确保了其本身的可靠性，所以误差更多来自于弱的能力。但是可能隐患是破坏了强的能力，因此需要思考如何将梯度隔离，只优化和弱的能力相关的token。也许可以detach ？

如果1到2的过程不对，或者2到3的过程不对，那么说明理解和生成能力之间有差距。
理解的起点是image encoder的representation，在理解的过程中逐渐分布转移到了能够被理解的space。而生成则是从噪音中获得了representation。这是两种from distribution to distribution的转移过程。
两种转移过程相互之间是隔离的，割裂的。

目前的UMM，理解和生成任务采用了不同的representation。对于理解任务，使用image encoder获得representation，称为RE(Representation of Encoder)。
对于生成任务，使用VAE获得representation，称为RG(Representation of Generation)。
理解任务其实是模型将RE转变为了某种模型能够理解的（也许是和文本的representation处于比较近的距离）representation，因为最终输出文本描述，所以这种文本的representation可以叫做RT(Representation of Text)。
生成任务是模型将noise和RT转变为了RG。
这两种任务其实需要模型有两种不同的representation transfer的能力。
这两种representation transfer可以被表述为：RE -> RT , RT -> RG。
RecA这篇论文通过输入RE来要求输出RG，实际上建立了一种新的representation transfer： RE -> RG。并发现这种post training对于生成任务有效果提升，相当于增强了RT -> RG。
这说明RE -> RG 和 RT -> RG两种representation transfer之间有某种正向相关性（也许可以证明二者的期望之间有某种正向相关性）
但是，总体上理解和生成任务的representation transfer过程还是割裂的，相互之间关系不大。对于同一张图片，其理解和生成的RE和RG相差很大，representation transfer过程也相差很大。
我希望通过某种方式能够拉近两种representation transfer之间的距离。
现在已有的工作：
图像理解和生成本来是两个领域。图像理解领域，大家使用Dino或者siglip之类的ViT来获得图像的feature，并用于VLM进行图像理解。图像生成领域，大家通过Diffusion、flow matching等方法训练图像生成模型。但是24年以来，一些工作尝试拉近两个领域的距离。出现这种趋势的一个重要的原因可能是，两个领域的模型能力提升都出现了瓶颈期，通过提升数据质量和数量的方式来升级的难度变得越来越大，因此尝试借助其他领域的模型来获得训练上的帮助变成了一个符合直觉的思路。
用生成来帮助理解的主要思路是：通过某种生成式的方式来优化visual representation。
一种做法是：将ViT的vision feature作为生成模型的输入条件，训练Diffusion模型生成图片，并让梯度传导到ViT，来优化ViT的参数。这种思路期望通过对ViT间接优化，来让ViT获得某种更好的representation（比如feature中会包含更多图像细节信息）。
相关论文：
DEEM: Diffusion Models Serve as the Eyes of Large Language Models for Image Perception
DIVA: DIFFUSION FEEDBACK HELPS CLIP SEE BETTER 
用理解来帮助生成的主要思路是：用理解模型给出的representation来优化生成模型的representation。
主要做法是：生成模型在生成最终图像前，会产生一些中间representation。通过将中间representation与siglip或者Dino针对同一张图片的representation靠近，来使得生成模型的中间representation能够获得一些来自理解模型的优势，期望这种优势能够帮助生成模型更好地生成最终图像。
相关论文：
REPA: Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think
其中DEEM和DIVA是让RE -> RG的过程能够帮助到RE（或者说帮助到pixel values -> RE),REPA是让noise(and text) -> RG的过程中加入了RE作为约束，从而帮助生成更好的RG。
这两种工作都是某一representation transfer受到了其他representation transfer的过程或者结果的正向影响。
但是我认为更好的方式是受到了其他representation transfer的过程影响，然后让这个过程可以通过梯度传导的方式传递给某一representation transfer，
而不是直接拿其他representation transfer的结果来影响某一representation transfer。
现在，我希望能够在UMM中提升RE -> RT , RT -> RG。我参考上面的做法，现在有下面的思路：
通过引入RG -> RT, RG -> RE, RT -> RE 这三种representation transfer也许有效果。
同时，我希望能够从gradient dynamics的角度分析这种效果。首先对于UMM来说，理解和生成任务的联合训练本身就有难度。可以认为是两种任务在loss optimization时提供的gradients方向
很可能并不一致，这产生了冲突。因此，也许可以分析RG -> RT, RG -> RE, RT -> RE 这三种representation transfer 在post training时的gradients和原本的两种RE -> RT , RT -> RG
以及RecA的RE -> RG的gradients之间的关系，来找到为什么RecA有效果的原因，以及这额外的三种是否会同样起效果。
我的分析思路可以用四个问题来描述。
问题1：UMM是否存在理解和生成的能力不匹配的问题？具体表现为，是否存在下面的情况：
情况一：UMM根据文本生成了一张图片，但是不能针对生成的图片产生正确的文本。
情况二：或者UMM根据图片生成了文本，但是不能根据文本生成正确的图片。
问题2：这种能力不匹配意味着什么？
情况一是否意味着：RT -> RG GOOD，RE -> RT BAD
情况二是否意味着：RT -> RG BAD，RE -> RT GOOD
问题3：能否通过RG -> RT，RT -> RE，RG -> RE来缓解情况。即通过representation transfer的reversal过程，来帮助模型。
问题4：假如RG和RE都来自于同一个image encoder，是否会更好？因为理解和生成任务转变为了RE - RT 和 RT -> RE,这本身就是representation transfer的相互reversal。
（此时RE - RT 和 RT -> RE是否能够被视为同一种任务了）
如果问题3的答案是yes，那么问题4的答案也是yes。
问题5：现在我寻找到一些例子，能够体现UMM的情况一和情况二。我是否有一些比较巧妙的思路，可以从模型生成的回答中采样出这两种特殊情况的数据。
或者有没有相关的论文是构建或者采样类似的数据的。
又或者，我能否引入更复杂的representation transfer。RT -> RG -> RT, RE -> RG -> RE, RE -> RT -> RE, 第一阶段需要是已有的，这样给出的结果比较稳定。
或者RG -> RT -> RG, RG -> RE -> RG, RT -> RE -> RT,这样梯度可以传导到第一阶段。但是先从一阶段的representation transfer开始。
梯度分析：假如某几种transfer过程的梯度表现相似，说明可以相互促进或者替代。否则就有害。

【Scaling Language-Centric Omnimodal
Representation Learning】不同模态embedding的各向异性反映了representation space的能力。各向异性越低，说明不同embeding的区分越好。
我认为，衡量不同embedding的变化过程，而不仅仅是孤立的某个layer的embedding。也许是一个更好的方式。
论文中一个有意思的现象是，只调整text的embedding space，可以对其他模态也减少各向异性。这说明不同模态之间有某种隐式的space alignment。

【Transfer between Modalities with MetaQueries】metaquery 并没有实现理解和生成之间的交互。交互需要两种任务参数相同。metaquery中
MLLM提供了一种static的embedding作为生成的condition。UMM中，理解和生成相互之间提供的应该是dynamic的representation transfer。

应该通过强化学习的方式来促进UMM，因为已有的feature足够好，需要自行探索相互之间的促进作用。思考RECA如何使用强化学习的方法。

【VAVAE】不仅让latent z和image encoder的embedding相似，也让z1和z2的相似度和embedding1和embedding2之间的相似度接近。
既能够让z align with encoder，也能够让不同z的feature之间的特征类似encoder。确保了feature space上的整体分布能够和encoder接近。（虽然这种接近不一定有好处）

【SRUM: FINE-GRAINED SELF-REWARDING FOR UNIFIED MULTIMODAL MODELS】通过模型自己给生成的图片打分，获得score。
但是理解生成更好的相互促进的方式，应该是in context improvement。即理解部分可以不断给出生成图像的修改意见，类似cot。要在训练的过程中
通过数据的形式构造出理解和生成的统一pattern。
能否先根据text生成image，然后输入image和text继续生成image。相当于对第一次生成的image做edit。
